# -*- coding: utf-8 -*-
"""DISTRIBUTED DATA  PROCESSING.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u5UA9Cdf5Q15sIMFqwcx7ZBSIAkY1SgX
"""

# Install Java
!apt-get install openjdk-11-jdk-headless -qq > /dev/null

# Download and extract Spark 3.4.1 with Hadoop 3
!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
!tar -xzf spark-3.4.1-bin-hadoop3.tgz

# Install findspark
!pip install -q findspark

import os
import findspark

# Set environment paths
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"

findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count

spark = SparkSession.builder \
    .appName("SparkColabAnalysis") \
    .getOrCreate()

# Continue with reading the CSV and running transformations...

user_id,transaction_id,amount,category,timestamp
1,tx001,250,groceries,2024-01-01 10:00:00
2,tx002,80,entertainment,2024-01-01 11:00:00
3,tx003,150,fashion,2024-01-01 12:30:00
4,tx004,120,groceries,2024-01-02 09:00:00
5,tx005,300,electronics,2024-01-03 14:00:00

# Save the data to a CSV file
csv_data = """user_id,transaction_id,amount,category,timestamp
1,tx001,250,groceries,2024-01-01 10:00:00
2,tx002,80,entertainment,2024-01-01 11:00:00
3,tx003,150,fashion,2024-01-01 12:30:00
4,tx004,120,groceries,2024-01-02 09:00:00
5,tx005,300,electronics,2024-01-03 14:00:00
"""

with open("transactions.csv", "w") as f:
    f.write(csv_data)

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count

# Initialize Spark session
spark = SparkSession.builder \
    .appName("LargeDatasetAnalysis") \
    .getOrCreate()

# Load dataset
df = spark.read.csv("transactions.csv", header=True, inferSchema=True)

# Filtering: Transactions where amount > 100
filtered_df = df.filter(col("amount") > 100)

# Grouping and aggregation by category
result_df = filtered_df.groupBy("category").agg(
    count("*").alias("transaction_count"),
    sum("amount").alias("total_amount"),
    avg("amount").alias("average_amount")
)

# Show analysis result
result_df.show()

# Stop session
spark.stop()

